node-id,type,id,name,abstract,categories,time,authors
Paper1,Paper,2006.00572,Improve Document Embedding for Text Categorization Through Deep Siamese Neural Network,"Due to the increasing amount of data on the internet, finding a highly-informative, low-dimensional representation for text is one of the main challenges for efficient natural language processing tasks including text classification. This representation should capture the semantic information of the text while retaining their relevance level for document classification. This approach maps the documents with similar topics to a similar space in vector space representation. To obtain representation for large text, we propose the utilization of deep Siamese neural networks. To embed document relevance in topics in the distributed representation, we use a Siamese neural network to jointly learn document representations. Our Siamese network consists of two sub-network of multi-layer perceptron. We examine our representation for the text categorization task on BBC news dataset. The results show that the proposed representations outperform the conventional and state-of-the-art representations in the text classification task on this dataset.",cs.CL cs.IR cs.LG,2020,"Erfaneh Gharavi, Hadi Veisi"
Technology1,Technology,Technology_Deep_Siamese_Neural_Network,Deep Siamese Neural Network,,,
Method1,Method,Method_Through_Deep_Siamese_Neural_Network,Through Deep Siamese Neural Network,,,
Task1,Task,Task_text_classification,text classification,,,
Paper2,Paper,2408.00612,Downstream bias mitigation is all you need,"The advent of transformer-based architectures and large language models (LLMs) have significantly advanced the performance of natural language processing (NLP) models. Since these LLMs are trained on huge corpuses of data from the web and other sources, there has been a major concern about harmful prejudices that may potentially be transferred from the data. In many applications, these pre-trained LLMs are fine-tuned on task specific datasets, which can further contribute to biases. This paper studies the extent of biases absorbed by LLMs during pre-training as well as task-specific behaviour after fine-tuning. We found that controlled interventions on pre-trained LLMs, prior to fine-tuning, have minimal effect on lowering biases in classifiers. However, the biases present in domain-specific datasets play a much bigger role, and hence mitigating them at this stage has a bigger impact. While pre-training does matter, but after the model has been pre-trained, even slight changes to co-occurrence rates in the fine-tuning dataset has a significant effect on the bias of the model.",cs.CL,2024,"Arkadeep Baksi, Rahul Singh, Tarun Joshi"
Task2,Task,Task_task,task,,,
Paper3,Paper,2110.14636,Pay attention to emoji: Feature Fusion Network with EmoGraph2vec Model for Sentiment Analysis,"With the explosive growth of social media, opinionated postings with emojis have increased explosively. Many emojis are used to express emotions, attitudes, and opinions. Emoji representation learning can be helpful to improve the performance of emoji-related natural language processing tasks, especially in text sentiment analysis. However, most studies have only utilized the fixed descriptions provided by the Unicode Consortium without consideration of actual usage scenarios. As for the sentiment analysis task, many researchers ignore the emotional impact of the interaction between text and emojis. It results that the emotional semantics of emojis cannot be fully explored. In this work, we propose a method called EmoGraph2vec to learn emoji representations by constructing a co-occurrence graph network from social data and enriching the semantic information based on an external knowledge base EmojiNet to embed emoji nodes. Based on EmoGraph2vec model, we design a novel neural network to incorporate text and emoji information into sentiment analysis, which uses a hybrid-attention module combined with TextCNN-based classifier to improve performance. Experimental results show that the proposed model can outperform several baselines for sentiment analysis on benchmark datasets. Additionally, we conduct a series of ablation and comparison experiments to investigate the effectiveness and interpretability of our model.",cs.CL,2022,"Xiaowei Yuan, Jingyuan Hu, Xiaodan Zhang, Honglei Lv"
Technology2,Technology,Technology_EmoGraph2vec,EmoGraph2vec,,,
Method2,Method,Method_EmoGraph2vec,EmoGraph2vec,,,
Task3,Task,Task_sentiment_analysis,sentiment analysis,,,
Paper4,Paper,2010.12638,Posterior Differential Regularization with f-divergence for Improving Model Robustness,"We address the problem of enhancing model robustness through regularization. Specifically, we focus on methods that regularize the model posterior difference between clean and noisy inputs. Theoretically, we provide a connection of two recent methods, Jacobian Regularization and Virtual Adversarial Training, under this framework. Additionally, we generalize the posterior differential regularization to the family of $f$-divergences and characterize the overall regularization framework in terms of Jacobian matrix. Empirically, we systematically compare those regularizations and standard BERT training on a diverse set of tasks to provide a comprehensive profile of their effect on model in-domain and out-of-domain generalization. For both fully supervised and semi-supervised settings, our experiments show that regularizing the posterior differential with $f$-divergence can result in well-improved model robustness. In particular, with a proper $f$-divergence, a BERT-base model can achieve comparable generalization as its BERT-large counterpart for in-domain, adversarial and domain shift scenarios, indicating the great potential of the proposed framework for boosting model generalization for NLP models.",cs.CL cs.LG stat.ML,2021,"Hao Cheng, Xiaodong Liu, Lis Pereira, Yaoliang Yu, Jianfeng Gao"
Technology5,Technology,Technology_Virtual_Adversarial_Training,Virtual Adversarial Training,,,
Method3,Method,Method_Jacobian_Regularization,Jacobian Regularization,,,
Method4,Method,Method_Virtual_Adversarial_Training,Virtual Adversarial Training,,,
Task4,Task,Task_Posterior_Differential_Regularization_with_f-divergence_for_Improving_Model_Robustness,Posterior Differential Regularization with f-divergence for Improving Model Robustness,,,
Task5,Task,Task_Virtual_Adversarial_Training,Virtual Adversarial Training,,,
Task6,Task,Task_Jacobian_Regularization,Jacobian Regularization,,,
Paper5,Paper,2305.12000,Deep Learning Approaches to Lexical Simplification: A Survey,"Lexical Simplification (LS) is the task of replacing complex for simpler words in a sentence whilst preserving the sentence's original meaning. LS is the lexical component of Text Simplification (TS) with the aim of making texts more accessible to various target populations. A past survey (Paetzold and Specia, 2017) has provided a detailed overview of LS. Since this survey, however, the AI/NLP community has been taken by storm by recent advances in deep learning, particularly with the introduction of large language models (LLM) and prompt learning. The high performance of these models sparked renewed interest in LS. To reflect these recent advances, we present a comprehensive survey of papers published between 2017 and 2023 on LS and its sub-tasks with a special focus on deep learning. We also present benchmark datasets for the future development of LS systems.",cs.CL,2023,"Kai North, Tharindu Ranasinghe, Matthew Shardlow, Marcos Zampieri"
Technology6,Technology,Technology_LS,LS,,,
Task7,Task,Task_Deep_Learning_Approaches_to_Lexical_Simplification:_A_Survey,Deep Learning Approaches to Lexical Simplification: A Survey,,,
Paper6,Paper,2202.13649,GausSetExpander: A Simple Approach for Entity Set Expansion,"Entity Set Expansion is an important NLP task that aims at expanding a small set of entities into a larger one with items from a large pool of candidates. In this paper, we propose GausSetExpander, an unsupervised approach based on optimal transport techniques. We propose to re-frame the problem as choosing the entity that best completes the seed set. For this, we interpret a set as an elliptical distribution with a centroid which represents the mean and a spread that is represented by the scale parameter. The best entity is the one that increases the spread of the set the least. We demonstrate the validity of our approach by comparing to state-of-the art approaches.",cs.CL,2023,"A\""issatou Diallo and Johannes F\""urnkranz"
Task8,Task,Task_Entity_Set_Expansion,Entity Set Expansion,,,
Paper7,Paper,2311.01684,CASE: Commonsense-Augmented Score with an Expanded Answer Space,"LLMs have demonstrated impressive zero-shot performance on NLP tasks thanks to the knowledge they acquired in their training. In multiple-choice QA tasks, the LM probabilities are used as an imperfect measure of the plausibility of each answer choice. One of the major limitations of the basic score is that it treats all words as equally important. We propose CASE, a Commonsense-Augmented Score with an Expanded Answer Space. CASE addresses this limitation by assigning importance weights for individual words based on their semantic relations to other words in the input. The dynamic weighting approach outperforms basic LM scores, not only because it reduces noise from unimportant words, but also because it informs the model of implicit commonsense knowledge that may be useful for answering the question. We then also follow prior work in expanding the answer space by generating lexically-divergent answers that are conceptually-similar to the choices. When combined with answer space expansion, our method outperforms strong baselines on 5 commonsense benchmarks. We further show these two approaches are complementary and may be especially beneficial when using smaller LMs.",cs.CL,2023,Wenkai Chen and Sahithya Ravi and Vered Shwartz
Technology8,Technology,Technology_LMs,LMs,,,
Method5,Method,Method_LMs,LMs,,,
Method6,Method,Method_answer_space_expansion,answer space expansion,,,
Paper8,Paper,1910.07333,A Probabilistic Framework for Learning Domain Specific Hierarchical Word Embeddings,"The meaning of a word often varies depending on its usage in different domains. The standard word embedding models struggle to represent this variation, as they learn a single global representation for a word. We propose a method to learn domain-specific word embeddings, from text organized into hierarchical domains, such as reviews in an e-commerce website, where products follow a taxonomy. Our structured probabilistic model allows vector representations for the same word to drift away from each other for distant domains in the taxonomy, to accommodate its domain-specific meanings. By learning sets of domain-specific word representations jointly, our model can leverage domain relationships, and it scales well with the number of domains. Using large real-world review datasets, we demonstrate the effectiveness of our model compared to state-of-the-art approaches, in learning domain-specific word embeddings that are both intuitive to humans and benefit downstream NLP tasks.",cs.CL cs.LG,2019,"Lahari Poddar, Gyorgy Szarvas, Lea Frermann"
Paper9,Paper,2403.06003,A Generalized Acquisition Function for Preference-based Reward Learning,"Preference-based reward learning is a popular technique for teaching robots and autonomous systems how a human user wants them to perform a task. Previous works have shown that actively synthesizing preference queries to maximize information gain about the reward function parameters improves data efficiency. The information gain criterion focuses on precisely identifying all parameters of the reward function. This can potentially be wasteful as many parameters may result in the same reward, and many rewards may result in the same behavior in the downstream tasks. Instead, we show that it is possible to optimize for learning the reward function up to a behavioral equivalence class, such as inducing the same ranking over behaviors, distribution over choices, or other related definitions of what makes two rewards similar. We introduce a tractable framework that can capture such definitions of similarity. Our experiments in a synthetic environment, an assistive robotics environment with domain transfer, and a natural language processing problem with real datasets demonstrate the superior performance of our querying method over the state-of-the-art information gain method.",cs.RO cs.AI cs.LG,2024,"Evan Ellis, Gaurav R. Ghosal, Stuart J. Russell, Anca Dragan, Erdem B{\i}y{\i}k"
Method7,Method,Method_A_Generalized_Acquisition_Function_for_Preference-based_Reward_Learning,A Generalized Acquisition Function for Preference-based Reward Learning,,,
Method8,Method,Method_actively_synthesizing_preference_queries,actively synthesizing preference queries,,,
Method9,Method,Method_querying_method,querying method,,,
Paper10,Paper,2310.15317,Exploring the Potential of Large Language Models in Generating Code-Tracing Questions for Introductory Programming Courses,"In this paper, we explore the application of large language models (LLMs) for generating code-tracing questions in introductory programming courses. We designed targeted prompts for GPT4, guiding it to generate code-tracing questions based on code snippets and descriptions. We established a set of human evaluation metrics to assess the quality of questions produced by the model compared to those created by human experts. Our analysis provides insights into the capabilities and potential of LLMs in generating diverse code-tracing questions. Additionally, we present a unique dataset of human and LLM-generated tracing questions, serving as a valuable resource for both the education and NLP research communities. This work contributes to the ongoing dialogue on the potential uses of LLMs in educational settings.",cs.CL cs.CY,2023,"Aysa Xuemo Fan, Ranran Haoran Zhang, Luc Paquette, Rui Zhang"
Technology9,Technology,Technology_LLMs,LLMs,,,
Task9,Task,Task_GPT4,GPT4,,,
Paper11,Paper,2304.14489,Automatic Generation of Labeled Data for Video-Based Human Pose Analysis via NLP applied to YouTube Subtitles,"With recent advancements in computer vision as well as machine learning (ML), video-based at-home exercise evaluation systems have become a popular topic of current research. However, performance depends heavily on the amount of available training data. Since labeled datasets specific to exercising are rare, we propose a method that makes use of the abundance of fitness videos available online. Specifically, we utilize the advantage that videos often not only show the exercises, but also provide language as an additional source of information. With push-ups as an example, we show that through the analysis of subtitle data using natural language processing (NLP), it is possible to create a labeled (irrelevant, relevant correct, relevant incorrect) dataset containing relevant information for pose analysis. In particular, we show that irrelevant clips ($n=332$) have significantly different joint visibility values compared to relevant clips ($n=298$). Inspecting cluster centroids also show different poses for the different classes.",cs.CV cs.LG eess.IV,2023,"Sebastian Dill, Susi Zhihan, Maurice Rohr, Maziar Sharbafi, Christoph Hoog Antink"
Technology10,Technology,Technology_ML,ML,,,
Technology11,Technology,Technology_NLP,NLP,,,
Method10,Method,Method_Automatic_Generation_of_Labeled_Data_for_Video-Based_Human_Pose_Analysis,Automatic Generation of Labeled Data for Video-Based Human Pose Analysis,,,
Method11,Method,Method_push-ups,push-ups,,,
Method12,Method,Method_natural_language_processing,natural language processing,,,
Task10,Task,Task_Automatic_Generation_of_Labeled_Data_for_Video-Based_Human_Pose_Analysis,Automatic Generation of Labeled Data for Video-Based Human Pose Analysis,,,
Paper12,Paper,2207.05737,How Do Multilingual Encoders Learn Cross-lingual Representation?,"NLP systems typically require support for more than one language. As different languages have different amounts of supervision, cross-lingual transfer benefits languages with little to no training data by transferring from other languages. From an engineering perspective, multilingual NLP benefits development and maintenance by serving multiple languages with a single system. Both cross-lingual transfer and multilingual NLP rely on cross-lingual representations serving as the foundation. As BERT revolutionized representation learning and NLP, it also revolutionized cross-lingual representations and cross-lingual transfer. Multilingual BERT was released as a replacement for single-language BERT, trained with Wikipedia data in 104 languages. Surprisingly, without any explicit cross-lingual signal, multilingual BERT learns cross-lingual representations in addition to representations for individual languages. This thesis first shows such surprising cross-lingual effectiveness compared against prior art on various tasks. Naturally, it raises a set of questions, most notably how do these multilingual encoders learn cross-lingual representations. In exploring these questions, this thesis will analyze the behavior of multilingual models in a variety of settings on high and low resource languages. We also look at how to inject different cross-lingual signals into multilingual encoders, and the optimization behavior of cross-lingual transfer with these models. Together, they provide a better understanding of multilingual encoders on cross-lingual transfer. Our findings will lead us to suggested improvements to multilingual encoders and cross-lingual transfer.",cs.CL,2022,Shijie Wu
Technology12,Technology,Technology_Wikipedia,Wikipedia,,,
Paper13,Paper,2107.10474,Back-Translated Task Adaptive Pretraining: Improving Accuracy and Robustness on Text Classification,"Language models (LMs) pretrained on a large text corpus and fine-tuned on a downstream text corpus and fine-tuned on a downstream task becomes a de facto training strategy for several natural language processing (NLP) tasks. Recently, an adaptive pretraining method retraining the pretrained language model with task-relevant data has shown significant performance improvements. However, current adaptive pretraining methods suffer from underfitting on the task distribution owing to a relatively small amount of data to re-pretrain the LM. To completely use the concept of adaptive pretraining, we propose a back-translated task-adaptive pretraining (BT-TAPT) method that increases the amount of task-specific data for LM re-pretraining by augmenting the task data using back-translation to generalize the LM to the target task domain. The experimental results show that the proposed BT-TAPT yields improved classification accuracy on both low- and high-resource data and better robustness to noise than the conventional adaptive pretraining method.",cs.CL cs.LG,2021,"Junghoon Lee, Jounghee Kim, Pilsung Kang"
Method13,Method,Method_back-translated_task-adaptive_pretraining,back-translated task-adaptive pretraining,,,
Method14,Method,Method_BT-TAPT,BT-TAPT,,,
Paper14,Paper,1910.13532,U-net architectures for fast prediction of incompressible laminar flows,"Machine learning is a popular tool that is being applied to many domains, from computer vision to natural language processing. It is not long ago that its use was extended to physics, but its capabilities remain to be accurately contoured. In this paper, we are interested in the prediction of 2D velocity and pressure fields around arbitrary shapes in laminar flows using supervised neural networks. To this end, a dataset composed of random shapes is built using Bezier curves, each shape being labeled with its pressure and velocity fields by solving Navier-Stokes equations using a CFD solver. Then, several U-net architectures are trained on the latter dataset, and their predictive efficiency is assessed on unseen shapes, using ad hoc error functions.",physics.comp-ph eess.IV,2019,"Junfeng Chen, Jonathan Viquerat, Elie Hachem"
Technology13,Technology,Technology_Navier-Stokes,Navier-Stokes,,,
Paper15,Paper,2001.08333,Applying Recent Innovations from NLP to MOOC Student Course Trajectory Modeling,"This paper presents several strategies that can improve neural network-based predictive methods for MOOC student course trajectory modeling, applying multiple ideas previously applied to tackle NLP (Natural Language Processing) tasks. In particular, this paper investigates LSTM networks enhanced with two forms of regularization, along with the more recently introduced Transformer architecture.",cs.LG cs.CY stat.ML,2020,"Clarence Chen, Zachary Pardos"
Technology14,Technology,Technology_LSTM,LSTM,,,
Paper16,Paper,2412.15712,Contrastive Learning for Task-Independent SpeechLLM-Pretraining,"Large language models (LLMs) excel in natural language processing but adapting these LLMs to speech processing tasks efficiently is not straightforward. Direct task-specific fine-tuning is limited by overfitting risks, data requirements, and computational costs. To address these challenges, we propose a scalable, two-stage training approach: (1) A task-independent speech pretraining stage using contrastive learning to align text and speech representations over all layers, followed by (2) a task-specific fine-tuning stage requiring minimal data. This approach outperforms traditional ASR pretraining and enables the model to surpass models specialized on speech translation and question answering while being trained on only 10% of the task-specific data.",cs.CL cs.HC,2024,"Maike Z\""ufle, Jan Niehues"
Method15,Method,Method_A_task-independent_speech_pretraining_stage_using_contrastive_learning,A task-independent speech pretraining stage using contrastive learning,,,
Paper17,Paper,2402.11235,ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs,"With the development of foundation models such as large language models, zero-shot transfer learning has become increasingly significant. This is highlighted by the generative capabilities of NLP models like GPT-4, and the retrieval-based approaches of CV models like CLIP, both of which effectively bridge the gap between seen and unseen data. In the realm of graph learning, the continuous emergence of new graphs and the challenges of human labeling also amplify the necessity for zero-shot transfer learning, driving the exploration of approaches that can generalize across diverse graph data without necessitating dataset-specific and label-specific fine-tuning. In this study, we extend such paradigms to zero-shot transferability in graphs by introducing ZeroG, a new framework tailored to enable cross-dataset generalization. Addressing the inherent challenges such as feature misalignment, mismatched label spaces, and negative transfer, we leverage a language model to encode both node attributes and class semantics, ensuring consistent feature dimensions across datasets. We also propose a prompt-based subgraph sampling module that enriches the semantic information and structure information of extracted subgraphs using prompting nodes and neighborhood aggregation, respectively. We further adopt a lightweight fine-tuning strategy that reduces the risk of overfitting and maintains the zero-shot learning efficacy of the language model. The results underscore the effectiveness of our model in achieving significant cross-dataset zero-shot transferability, opening pathways for the development of graph foundation models. Codes and data are available at https://github.com/NineAbyss/ZeroG.",cs.LG,2024,"Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, Jia Li"
Technology15,Technology,Technology_GPT-4,GPT-4,,,
Technology16,Technology,Technology_CLIP,CLIP,,,
Technology18,Technology,Technology_ZeroG,ZeroG,,,
Method16,Method,Method_CLIP,CLIP,,,
Method17,Method,Method_ZeroG,ZeroG,,,
Task11,Task,Task_Investigating_Cross-dataset_Zero-shot_Transferability_in_Graphs,Investigating Cross-dataset Zero-shot Transferability in Graphs,,,
Paper18,Paper,2310.02655,AGIR: Automating Cyber Threat Intelligence Reporting with Natural Language Generation,"Cyber Threat Intelligence (CTI) reporting is pivotal in contemporary risk management strategies. As the volume of CTI reports continues to surge, the demand for automated tools to streamline report generation becomes increasingly apparent. While Natural Language Processing techniques have shown potential in handling text data, they often struggle to address the complexity of diverse data sources and their intricate interrelationships. Moreover, established paradigms like STIX have emerged as de facto standards within the CTI community, emphasizing the formal categorization of entities and relations to facilitate consistent data sharing. In this paper, we introduce AGIR (Automatic Generation of Intelligence Reports), a transformative Natural Language Generation tool specifically designed to address the pressing challenges in the realm of CTI reporting. AGIR's primary objective is to empower security analysts by automating the labor-intensive task of generating comprehensive intelligence reports from formal representations of entity graphs. AGIR utilizes a two-stage pipeline by combining the advantages of template-based approaches and the capabilities of Large Language Models such as ChatGPT. We evaluate AGIR's report generation capabilities both quantitatively and qualitatively. The generated reports accurately convey information expressed through formal language, achieving a high recall value (0.99) without introducing hallucination. Furthermore, we compare the fluency and utility of the reports with state-of-the-art approaches, showing how AGIR achieves higher scores in terms of Syntactic Log-Odds Ratio (SLOR) and through questionnaires. By using our tool, we estimate that the report writing time is reduced by more than 40%, therefore streamlining the CTI production of any organization and contributing to the automation of several CTI tasks.",cs.CR cs.CL,2023,"Filippo Perrina, Francesco Marchiori, Mauro Conti, Nino Vincenzo Verde"
Technology19,Technology,Technology_Natural_Language_Processing,Natural Language Processing,,,
Technology20,Technology,Technology_Automatic_Generation_of_Intelligence_Reports,Automatic Generation of Intelligence Reports,,,
Technology21,Technology,Technology_Large_Language_Models,Large Language Models,,,
Method18,Method,Method_questionnaires,questionnaires,,,
Task12,Task,Task_AGIR,AGIR,,,
Task13,Task,Task_Automating_Cyber_Threat_Intelligence_Reporting_with_Natural_Language_Generation,Automating Cyber Threat Intelligence Reporting with Natural Language Generation,,,
Paper19,Paper,2012.15562,UNKs Everywhere: Adapting Multilingual Language Models to New Scripts,"Massively multilingual language models such as multilingual BERT offer state-of-the-art cross-lingual transfer performance on a range of NLP tasks. However, due to limited capacity and large differences in pretraining data sizes, there is a profound performance gap between resource-rich and resource-poor target languages. The ultimate challenge is dealing with under-resourced languages not covered at all by the models and written in scripts unseen during pretraining. In this work, we propose a series of novel data-efficient methods that enable quick and effective adaptation of pretrained multilingual models to such low-resource languages and unseen scripts. Relying on matrix factorization, our methods capitalize on the existing latent knowledge about multiple languages already available in the pretrained model's embedding matrix. Furthermore, we show that learning of the new dedicated embedding matrix in the target language can be improved by leveraging a small number of vocabulary items (i.e., the so-called lexically overlapping tokens) shared between mBERT's and target language vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can yield improvements for low-resource languages written in scripts covered by the pretrained model.",cs.CL,2021,"Jonas Pfeiffer, Ivan Vuli\'c, Iryna Gurevych, Sebastian Ruder"
Method19,Method,Method_matrix_factorization,matrix factorization,,,
Paper20,Paper,2402.12880,Autism Detection in Speech -- A Survey,"There has been a range of studies of how autism is displayed in voice, speech, and language. We analyse studies from the biomedical, as well as the psychological domain, but also from the NLP domain in order to find linguistic, prosodic and acoustic cues that could indicate autism. Our survey looks at all three domains. We define autism and which comorbidities might influence the correct detection of the disorder. We especially look at observations such as verbal and semantic fluency, prosodic features, but also disfluencies and speaking rate. We also show word-based approaches and describe machine learning and transformer-based approaches both on the audio data as well as the transcripts. Lastly, we conclude, while there already is a lot of research, female patients seem to be severely under-researched. Also, most NLP research focuses on traditional machine learning methods instead of transformers which could be beneficial in this context. Additionally, we were unable to find research combining both features from audio and transcripts.",cs.CL,2024,Nadine Probol and Margot Mieskes
Method20,Method,Method_machine_learning,machine learning,,,
Method21,Method,Method_transformers,transformers,,,
